
generating certs will have to be done for each environmen or cluster as IPs are in cert request

openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=127.0.0.1" -days 10000 -out ca.crt
openssl genrsa -out apiserver.key 2048
openssl x509 -req -in apiserver.key -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver.crt -days 10000 -extensions v3_ext -extfile csr.conf
openssl req -newkey rsa:2048 -nodes -keyout sa.key -x509 -days 10000 -out sa.pub

openssl req -newkey rsa:2048 -nodes -keyout apiserver-kubelet-client.key -x509 -days 10000 -out apiserver-kubelet-client.crt

kubelet client cert generation
    ca.crt from master needs to go to node
    openssl x509 -req -in node1.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out node1.crt -days 10000
    node1 is still in notReady state and I think it has to do with api server not trusting this cert and kubelet 10250 port with host IP maybe cert needs specific host IP

To Access Dashboard
kubectl -n kube-system edit service kubernetes-dashboard
    change ClusterIp to node port
kubectl get all --all-namespaces
    note port of kube-dashboard and then get vagrant VM ip and call

cluster-admin create kubeconfig file with embedded certs (this did not work for kube-dashboard used token based auth csv file instead)
    kubectl config set-cluster cluster.local --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=https://172.17.177.11:6443 --kubeconfig=admin.kubeconfig
    kubectl config set-credentials cluster-admin --client-certificate=/etc/kubernetes/pki/admin.crt --client-key=/etc/kubernetes/pki/admin.key --embed-certs=true --kubeconfig=admin.kubeconfig
    kubectl config set-context default --cluster=cluster.local --user=cluster-admin --kubeconfig=admin.kubeconfig

kubectl get logs problem
    when kubectl describe no node1 you get eth0 ip as internal IP which is vagrant assigned and this is based
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname # may be vagrant issue here with kubectl logs https://github.com/kubernetes/kubernetes/issues/45583
    putting manual --node-ip=   in kubelet args should solve this

kubelet node labels
    node-role.kubernetes.io/master=""
    node-role.kubernetes.io/worker=""

kube apiserver removed kube front proxy auth and headers
    # - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    # - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    # - --requestheader-group-headers=X-Remote-Group
    # - --requestheader-allowed-names=front-proxy-client
    # - --requestheader-username-headers=X-Remote-User
    # - --requestheader-extra-headers-prefix=X-Remote-Extra-
    # - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt

set different read permissions for /root/.kube/admin.key

do this commmand for all nodes that need to run fluentd
    kubectl label nodes master1 beta.kubernetes.io/fluentd-ds-ready=true

need to clean up fluentd config map

error when trying to access kibana
    is forbidden: User \"system:anonymous\" cannot get services/proxy in the namespace \"kube-system\
after setting node port and removing kibana base url env var
    http://172.17.177.12:30001/app/kibana#/discover?_g=()

not enough memory to run efk stack

elasticsearch mandatory sysctl entry
    vm.max_map_count=262144

prove nginx-ingress works
    install dnsmasq
        yum install dnsmasq
    add the follwing line to end of /etc/dnsmasq.conf or put in file in dnsmasq.d
        address=/kube.example.com/127.0.0.1
    start local dns server
        systemctl start dnsmasq
    test
        curl http://ngapp.kube.example.com:30080

still a bug in sysctl values in kube-node
    a certain amount of time needs to pass after ipv4 forward to put  in bridge call iptables